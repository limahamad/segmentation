--- venv/lib/python3.12/site-packages/torch/nn/modules/activation.py

+++ venv/lib/python3.12/site-packages/torch/nn/modules/activation.py
@@ -1,16 +1,17 @@
 class ReLU(Module):
-    r"""Applies the rectified linear unit function element-wise.
+    r"""Applies the rectified linear unit function element-wise:
 
-    :math:`\text{ReLU}(x) = (x)^+ = \max(0, x)`
+    :math:`\text{ReLU}(x)= \max(0, x)`
 
     Args:
         inplace: can optionally do the operation in-place. Default: ``False``
 
     Shape:
-        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.
-        - Output: :math:`(*)`, same shape as the input.
+        - Input: :math:`(N, *)` where `*` means, any number of additional
+          dimensions
+        - Output: :math:`(N, *)`, same shape as the input
 
-    .. image:: ../scripts/activation_images/ReLU.png
+    .. image:: scripts/activation_images/ReLU.png
 
     Examples::
 
@@ -23,20 +24,19 @@
 
         >>> m = nn.ReLU()
         >>> input = torch.randn(2).unsqueeze(0)
-        >>> output = torch.cat((m(input), m(-input)))
+        >>> output = torch.cat((m(input),m(-input)))
     """
+    __constants__ = ['inplace']
 
-    __constants__ = ['inplace']
-    inplace: bool
-
-    def __init__(self, inplace: bool = False):
-        super().__init__()
+    def __init__(self, inplace=False):
+        super(ReLU, self).__init__()
         self.inplace = inplace
 
-    def forward(self, input: Tensor) -> Tensor:
+    @weak_script_method
+    def forward(self, input):
         return F.relu(input, inplace=self.inplace)
 
-    def extra_repr(self) -> str:
-        inplace_str = 'inplace=True' if self.inplace else ''
+    def extra_repr(self):
+        inplace_str = 'inplace' if self.inplace else ''
         return inplace_str
 